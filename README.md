# biorag

## Motivation
Large Language Models (LLMs) are powerful tools for biomedical research, but they often **hallucinate** when facing very specific domain questions. In biomedical sciences, this is particularly critical: wrong gene-disease associations or invented citations can mislead research and waste valuable time.  
The challenge is how to **ground LLM answers in reliable biomedical evidence** and avoid hallucinations while keeping the flexibility of natural language queries.

## What is biorag?
**biorag** is a lightweight Retrieval-Augmented Generation (RAG) pipeline designed for biomedical question answering.  
It combines **BM25 lexical search** and **vector-based embeddings (FAISS + SentenceTransformers)** with local LLMs through **Ollama**.  
The goal is to ensure that model answers are supported by retrieved documents, reducing hallucination risks.

## File overview
- **`orquestador.py`** â†’ main entrypoint, runs the full pipeline.  
- **`rag_index.py`** â†’ builds and manages BM25 + vector indices.  
- **`retriever.py`** â†’ retrieves top relevant snippets from the indices.  
- **`vector_store.py`** â†’ manages embeddings and FAISS storage.  
- **`generador.py`** â†’ generates the final answer, formatting with citations.  
- **`README.md`** â†’ project documentation.  

## How to use biorag

1. Open a terminal in the project directory.  
2. Run:  
   python orquestador.py
3. Wait for the pipeline to initialize.
4. Open the local interface at:
ðŸ‘‰ http://127.0.0.1:7862

# biorag

## MotivaciÃ³n
Los LLMs son herramientas poderosas para la investigaciÃ³n biomÃ©dica, pero suelen alucinar cuando se enfrentan a preguntas muy especÃ­ficas del dominio. En biomedicina esto es crÃ­tico: asociaciones falsas gen-enfermedad o citas inventadas pueden desviar la investigaciÃ³n y hacer perder tiempo.
El desafÃ­o es cÃ³mo anclar las respuestas de los LLM en evidencia biomÃ©dica confiable, evitando alucinaciones y manteniendo la flexibilidad de consultas en lenguaje natural.

## Â¿QuÃ© es biorag?
biorag es un pipeline sencillo de RAG (Retrieval-Augmented Generation) para preguntas biomÃ©dicas.
Combina bÃºsqueda lÃ©xica BM25 y vectores de embeddings (FAISS + SentenceTransformers) con LLMs locales a travÃ©s de Ollama.
El objetivo es que las respuestas estÃ©n siempre respaldadas por documentos recuperados, reduciendo el riesgo de alucinaciones.

## Archivos
- orquestador.py â†’ punto de entrada principal, ejecuta todo el flujo.

- rag_index.py â†’ construcciÃ³n y gestiÃ³n de Ã­ndices BM25 + vectores.

- retriever.py â†’ recuperaciÃ³n de snippets mÃ¡s relevantes.

- vector_store.py â†’ gestiÃ³n de embeddings y almacenamiento en FAISS.

- generador.py â†’ genera la respuesta final con citas.

- README.md â†’ documentaciÃ³n del proyecto.

## CÃ³mo usar

1. Abrir una terminal en el directorio del proyecto.
2. Ejecutar:
   python orquestador.py
3. Esperar que se inicialice el pipeline.
4. Entrar a la interfaz local en:
ðŸ‘‰ http://127.0.0.1:7862


