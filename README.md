# BioRAG ‚Äî Hybrid RAG sobre Europe PMC (bioRxiv)

Este proyecto implementa un pipeline RAG ‚Äúh√≠brido‚Äù (l√©xico + sem√°ntico) para:
1) buscar papers en Europe PMC (filtrando bioRxiv),
2) rankear candidatos combinando BM25 + embeddings (FAISS),
3) extraer snippets cortos como evidencia,
4) generar una respuesta en ingl√©s con citas [n] usando un modelo local v√≠a Ollama.

---

## Pipeline (end-to-end)

### A. Entrada del usuario
- El usuario escribe una pregunta (en espa√±ol o ingl√©s) en la UI (Gradio).

### B. Traducci√≥n/normalizaci√≥n de la consulta (para Europe PMC)
- Si la pregunta est√° en espa√±ol (o es ‚Äúlibre‚Äù), se convierte a una query en ingl√©s estilo Lucene (AND/OR/NOT, frases entre comillas, etc.).
- Esto se hace con un modelo local v√≠a Ollama (por defecto: `codellama:7b-instruct`) y un prompt few-shot.

**Objetivo:** producir una query robusta para el buscador de Europe PMC.

### C. B√∫squeda en Europe PMC
- Se consulta el endpoint de Europe PMC con la query generada.
- En tu implementaci√≥n actual se filtra a preprints de bioRxiv.
- Se descargan metadatos (t√≠tulo, abstract, a√±o, DOI, links) y, si existe PMCID, se intenta traer full-text en XML y parsearlo.

**Salida:** lista de documentos ‚ÄúDoc‚Äù normalizados.

### D. Index local (BM25 + embeddings) + persistencia
Se construye un √≠ndice local para esos documentos:

1) **BM25 (similitud l√©xica)**
- Se tokenizan los textos (title + abstract + fulltext) y se entrena BM25.

2) **Embeddings + FAISS (similitud contextual)**
- Se embebe cada documento con SentenceTransformers (default: `all-MiniLM-L6-v2`, embeddings normalizados).
- Se guardan/reusan embeddings en `./biorag_store/`:
  - `embeddings.npy` (vectores)
  - `meta.parquet` (metadatos + hash por texto normalizado)
- Con esos embeddings se construye un √≠ndice FAISS (`IndexFlatIP`) para b√∫squeda por producto interno (equivalente a coseno si est√°n normalizados).

**Objetivo:** no recalcular embeddings si el mismo texto ya exist√≠a (dedupe por hash).

### E. Ranking h√≠brido (score final)
Para rankear:
- Score l√©xico: BM25 sobre ‚Äúpalabras clave‚Äù extra√≠das desde la query Lucene.
- Score contextual: FAISS sobre embedding de la pregunta original.
- Score final: combinaci√≥n lineal
  - `alpha * score_vector + (1 - alpha) * score_bm25_normalizado`

### F. Extracci√≥n de evidencia (snippets)
- Para los top docs, se arma un snippet por documento (m√°x. N snippets).
- Se cortan oraciones del t√≠tulo+abstract y se priorizan oraciones con patrones t√≠picos (genes, p-values, n√∫meros, etc.).
- Cada snippet queda corto (ej. <=200 chars) para entrar f√°cil en el contexto del generador.

### G. Generaci√≥n (answer + referencias) con citas [n]
- Se construye un prompt con:
  - lista de snippets como contexto, cada l√≠nea termina con su cita [n]
  - instrucci√≥n: responder en ingl√©s y **solo** con evidencia del contexto, citando cada afirmaci√≥n con [n]
- Se llama a Ollama con un modelo local (por defecto: `gemma3:1b`).
- Se devuelve:
  - `answer` (con citas inline [n])
  - `refs` (lista formateada de referencias [n] con t√≠tulo, fuente, a√±o, link/DOI)

---

## Estructura del c√≥digo

- `orquestador.py`:
  - Une todo el pipeline y expone una UI en Gradio (entrada: ‚ÄúConsulta‚Äù, salida: query final, respuesta con citas, referencias y top recuperados).
- `rag_index.py`:
  - Traducci√≥n de consulta a Lucene, b√∫squeda en Europe PMC, normalizaci√≥n de resultados.
  - `Index_coincidencias`: BM25 + embeddings + FAISS + score h√≠brido.
- `vector_store.py`:
  - Persistencia de embeddings y metadatos en `./biorag_store/` (reuso + dedupe).
- `retriever.py`:
  - Split simple en oraciones y selecci√≥n de snippets ‚Äúrepresentativos‚Äù.
- `generador.py`:
  - Arma el prompt con citas y llama a Ollama de forma robusta (generate/chat).
  - Devuelve respuesta + bloque de referencias.

---

## C√≥mo correr
1) Instalar dependencias:
   - `pip install -r requirements.txt`
2) Tener Ollama corriendo y con modelos descargados:
   - Query: `codellama:7b-instruct`
   - Answer: `gemma3:1b`
3) Ejecutar:
   - `python orquestador.py`
4) Entrar a la interfaz local en:
üëâ http://127.0.0.1:7862


Notas:
- Par√°metros √∫tiles: `page_size`, `alpha`, `top_k`, `max_snippets`.
- Persistencia: se crea `./biorag_store/` autom√°ticamente.

## Arquitectura del RAG

![Arquitectura del RAG](img/rag_pipeline.png)

